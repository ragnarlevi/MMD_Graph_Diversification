{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import collections\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from numbers import Real\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.preprocessing import normalize as normalizer\n",
    "\n",
    "from grakel.graph import Graph\n",
    "from grakel.kernels import Kernel\n",
    "\n",
    "# Python 2/3 cross-compatibility import\n",
    "from six import itervalues\n",
    "from six import iteritems\n",
    "from six.moves import filterfalse\n",
    "\n",
    "\n",
    "def _dot(x, y):\n",
    "    return sum(x[k]*y[k] for k in x)\n",
    "\n",
    "class Propagation(Kernel):\n",
    "    r\"\"\"The Propagation kernel for fully labeled graphs.\n",
    "    See :cite:`neumann2015propagation`: Algorithms 1, 3, p. 216, 221.\n",
    "    Parameters\n",
    "    ----------\n",
    "    t_max : int, default=5\n",
    "        Maximum number of iterations.\n",
    "    w : int, default=0.01\n",
    "        Bin width.\n",
    "    M : str, default=\"TV\"\n",
    "        The preserved distance metric (on local sensitive hashing):\n",
    "            - \"H\": hellinger\n",
    "            - \"TV\": total-variation\n",
    "    metric : function (Counter, Counter -> number),\n",
    "        default=:math:`f(x,y)=\\sum_{i} x_{i}*y_{i}`\n",
    "        A metric between two 1-dimensional numpy arrays of numbers that outputs a number.\n",
    "        It must consider the case where the keys of y are not in x, when different features appear\n",
    "        at transform.\n",
    "    random_state :  RandomState or int, default=None\n",
    "        A random number generator instance or an int to initialize a RandomState as a seed.\n",
    "    Attributes\n",
    "    ----------\n",
    "    _enum_labels : dict\n",
    "        Holds the enumeration of the input labels.\n",
    "    _parent_labels : set\n",
    "        Holds a set of the input labels.\n",
    "    random_state_ : RandomState\n",
    "        A RandomState object handling all randomness of the class.\n",
    "    \"\"\"\n",
    "\n",
    "    _graph_format = \"adjacency\"\n",
    "    attr_ = False\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_jobs=None,\n",
    "                 verbose=False,\n",
    "                 normalize=False,\n",
    "                 random_state=None,\n",
    "                 metric=_dot,\n",
    "                 M=\"TV\",\n",
    "                 t_max=5,\n",
    "                 w=0.01):\n",
    "        \"\"\"Initialise a propagation kernel.\"\"\"\n",
    "        super(Propagation, self).__init__(n_jobs=n_jobs,\n",
    "                                          verbose=verbose,\n",
    "                                          normalize=normalize)\n",
    "\n",
    "        self.random_state = random_state\n",
    "        self.M = M\n",
    "        self.t_max = t_max\n",
    "        self.w = w\n",
    "        self.metric = metric\n",
    "        self._initialized.update({\"M\": False, \"t_max\": False, \"w\": False,\n",
    "                                  \"random_state\": False, \"metric\": False})\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize all transformer arguments, needing initialization.\"\"\"\n",
    "        super(Propagation, self).initialize()\n",
    "\n",
    "        if not self._initialized[\"random_state\"]:\n",
    "            self.random_state_ = check_random_state(self.random_state)\n",
    "            self._initialized[\"random_state\"] = True\n",
    "\n",
    "        if not self._initialized[\"metric\"]:\n",
    "            if (type(self.M) is not str or\n",
    "                    (self.M not in [\"H\", \"TV\"] and not self.attr_) or\n",
    "                    (self.M not in [\"L1\", \"L2\"] and self.attr_)):\n",
    "                if self.attr_:\n",
    "                    raise TypeError('Metric type must be a str, one of \"L1\", \"L2\"')\n",
    "                else:\n",
    "                    raise TypeError('Metric type must be a str, one of \"H\", \"TV\"')\n",
    "\n",
    "            if not self.attr_:\n",
    "                self.take_sqrt_ = self.M == \"H\"\n",
    "\n",
    "            self.take_cauchy_ = self.M in [\"TV\", \"L1\"]\n",
    "            self._initialized[\"metric\"] = True\n",
    "\n",
    "        if not self._initialized[\"t_max\"]:\n",
    "            if type(self.t_max) is not int or self.t_max <= 0:\n",
    "                raise TypeError('The number of iterations must be a ' +\n",
    "                                'positive integer.')\n",
    "            self._initialized[\"t_max\"] = True\n",
    "\n",
    "        if not self._initialized[\"w\"]:\n",
    "            if not isinstance(self.w, Real) and self.w <= 0:\n",
    "                raise TypeError('The bin width must be a positive number.')\n",
    "            self._initialized[\"w\"] = True\n",
    "\n",
    "        if not self._initialized[\"metric\"]:\n",
    "            if not callable(self.metric):\n",
    "                raise TypeError('The base kernel must be callable.')\n",
    "            self._initialized[\"metric\"] = True\n",
    "\n",
    "    def pairwise_operation(self, x, y):\n",
    "        \"\"\"Calculate the kernel value between two elements.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x, y: list\n",
    "            Inverse label dictionaries.\n",
    "        Returns\n",
    "        -------\n",
    "        kernel : number\n",
    "            The kernel value.\n",
    "        \"\"\"\n",
    "        return sum(self.metric(x[t], y[t]) for t in range(self.t_max))\n",
    "\n",
    "    def parse_input(self, X):\n",
    "        \"\"\"Parse and create features for the propation kernel.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : iterable\n",
    "            For the input to pass the test, we must have:\n",
    "            Each element must be an iterable with at most three features and at\n",
    "            least one. The first that is obligatory is a valid graph structure\n",
    "            (adjacency matrix or edge_dictionary) while the second is\n",
    "            node_labels and the third edge_labels (that correspond to the given\n",
    "            graph format). A valid input also consists of graph type objects.\n",
    "        Returns\n",
    "        -------\n",
    "        local_values : dict\n",
    "            A dictionary of pairs between each input graph and a bins where the\n",
    "            sampled graphlets have fallen.\n",
    "        \"\"\"\n",
    "        if not isinstance(X, collections.Iterable):\n",
    "            raise ValueError('input must be an iterable\\n')\n",
    "        else:\n",
    "            i = -1\n",
    "            transition_matrix = dict()\n",
    "            labels = set()\n",
    "            L = list()\n",
    "            for (idx, x) in enumerate(iter(X)):\n",
    "                is_iter = isinstance(x, collections.Iterable)\n",
    "                if is_iter:\n",
    "                    x = list(x)\n",
    "                if is_iter and len(x) in [0, 2, 3, 4]:\n",
    "                    if len(x) == 0:\n",
    "                        warnings.warn('Ignoring empty element on ' +\n",
    "                                      'index: '+str(idx))\n",
    "                        continue\n",
    "                    if len(x) == 2 and type(x[0]) is Graph:\n",
    "                        g, T = x\n",
    "                    else:\n",
    "                        g = Graph(x[0], x[1], {}, self._graph_format)\n",
    "                        if len(x) == 4:\n",
    "                            T = x[3]\n",
    "                        else:\n",
    "                            T = None\n",
    "                elif type(x) is Graph:\n",
    "                    g, T = x, None\n",
    "                else:\n",
    "                    raise ValueError('Each element of X must be either a ' +\n",
    "                                     'Graph or an iterable with at least 2 ' +\n",
    "                                     'and at most 4 elements\\n')\n",
    "\n",
    "                if T is not None:\n",
    "                    if T.shape[0] != T.shape[1]:\n",
    "                        raise TypeError('Transition matrix on index' +\n",
    "                                        ' ' + str(idx) + 'must be ' +\n",
    "                                        'a square matrix.')\n",
    "                    if T.shape[0] != g.nv():\n",
    "                        raise TypeError('Propagation matrix must ' +\n",
    "                                        'have the same dimension ' +\n",
    "                                        'as the number of vertices.')\n",
    "                else:\n",
    "                    T = g.get_adjacency_matrix()\n",
    "\n",
    "                i += 1\n",
    "                transition_matrix[i] = normalizer(T, axis=1, norm='l1')\n",
    "                label = g.get_labels(purpose='adjacency')\n",
    "                try:\n",
    "                    labels |= set(itervalues(label))\n",
    "                except TypeError:\n",
    "                    raise TypeError('For a non attributed kernel, labels should be hashable.')\n",
    "                L.append((g.nv(), label))\n",
    "\n",
    "            if i == -1:\n",
    "                raise ValueError('Parsed input is empty')\n",
    "\n",
    "            # The number of parsed graphs\n",
    "            n = i+1\n",
    "\n",
    "            # enumerate labels\n",
    "            if self._method_calling == 1:\n",
    "                enum_labels = {l: i for (i, l) in enumerate(list(labels))}\n",
    "                self._enum_labels = enum_labels\n",
    "                self._parent_labels = labels\n",
    "            elif self._method_calling == 3:\n",
    "                new_elements = labels - self._parent_labels\n",
    "                if len(new_elements) > 0:\n",
    "                    new_enum_labels = iter((l, i) for (i, l) in\n",
    "                                           enumerate(list(new_elements), len(self._enum_labels)))\n",
    "                    enum_labels = dict(chain(iteritems(self._enum_labels), new_enum_labels))\n",
    "                else:\n",
    "                    enum_labels = self._enum_labels\n",
    "\n",
    "            # make a matrix for all graphs that contains label vectors\n",
    "            P, data, indexes = dict(), list(), [0]\n",
    "            for (k, (nv, label)) in enumerate(L):\n",
    "                data += [(indexes[-1] + j, enum_labels[label[j]]) for j in range(nv)]\n",
    "                indexes.append(indexes[-1] + nv)\n",
    "\n",
    "            # Initialise the on hot vector\n",
    "            rows, cols = zip(*data)\n",
    "            P = np.zeros(shape=(indexes[-1], len(enum_labels)))\n",
    "            P[rows, cols] = 1\n",
    "            dim_orig = len(self._enum_labels)\n",
    "\n",
    "            # feature vectors\n",
    "            if self._method_calling == 1:\n",
    "                # simple normal\n",
    "                self._u, self._b, self._hd = list(), list(), list()\n",
    "                for t in range(self.t_max):\n",
    "                    u = self.random_state_.randn(len(enum_labels))\n",
    "\n",
    "                    if self.take_cauchy_:\n",
    "                        # cauchy\n",
    "                        u = np.divide(u, self.random_state_.randn(len(enum_labels)))\n",
    "\n",
    "                    self._u.append(u)\n",
    "                    # random offset\n",
    "                    self._b.append(self.w*self.random_state_.rand())\n",
    "\n",
    "                phi = {k: dict() for k in range(n)}\n",
    "                for t in range(self.t_max):\n",
    "                    # for hash all graphs inside P and produce the feature vectors\n",
    "                    hashes = self.calculate_LSH(P, self._u[t], self._b[t])\n",
    "                    hd = dict((j, i) for i, j in enumerate(set(np.unique(hashes))))\n",
    "                    self._hd.append(hd)\n",
    "                    features = np.vectorize(lambda i: hd[i])(hashes)\n",
    "\n",
    "                    # Accumulate the results.\n",
    "                    for k in range(n):\n",
    "                        phi[k][t] = Counter(features[indexes[k]:indexes[k+1]])\n",
    "\n",
    "                    # calculate the Propagation matrix if needed\n",
    "                    if t < self.t_max-1:\n",
    "                        for k in range(n):\n",
    "                            start, end = indexes[k:k+2]\n",
    "                            P[start:end, :] = np.dot(transition_matrix[k], P[start:end, :])\n",
    "\n",
    "                return [phi[k] for k in range(n)]\n",
    "\n",
    "            elif (self._method_calling == 3 and dim_orig >= len(enum_labels)):\n",
    "                phi = {k: dict() for k in range(n)}\n",
    "                for t in range(self.t_max):\n",
    "                    # for hash all graphs inside P and produce the feature vectors\n",
    "                    hashes = self.calculate_LSH(P, self._u[t], self._b[t])\n",
    "                    hd = dict(chain(\n",
    "                            iteritems(self._hd[t]),\n",
    "                            iter((j, i) for i, j in enumerate(\n",
    "                                    filterfalse(lambda x: x in self._hd[t],\n",
    "                                                np.unique(hashes)),\n",
    "                                    len(self._hd[t])))))\n",
    "\n",
    "                    features = np.vectorize(lambda i: hd[i])(hashes)\n",
    "\n",
    "                    # Accumulate the results.\n",
    "                    for k in range(n):\n",
    "                        phi[k][t] = Counter(features[indexes[k]:indexes[k+1]])\n",
    "\n",
    "                    # calculate the Propagation matrix if needed\n",
    "                    if t < self.t_max-1:\n",
    "                        for k in range(n):\n",
    "                            start, end = indexes[k:k+2]\n",
    "                            P[start:end, :] = np.dot(transition_matrix[k], P[start:end, :])\n",
    "\n",
    "                return [phi[k] for k in range(n)]\n",
    "\n",
    "            else:\n",
    "                cols = np.array(cols)\n",
    "                vertices = np.where(cols < dim_orig)[0]\n",
    "                vertices_p = np.where(cols >= dim_orig)[0]\n",
    "                nnv = len(enum_labels) - dim_orig\n",
    "                phi = {k: dict() for k in range(n)}\n",
    "                for t in range(self.t_max):\n",
    "                    # hash all graphs inside P and produce the feature vectors\n",
    "                    hashes = self.calculate_LSH(P[vertices, :dim_orig],\n",
    "                                                self._u[t], self._b[t])\n",
    "\n",
    "                    hd = dict(chain(\n",
    "                            iteritems(self._hd[t]),\n",
    "                            iter((j, i) for i, j in enumerate(\n",
    "                                    filterfalse(lambda x: x in self._hd[t],\n",
    "                                                np.unique(hashes)),\n",
    "                                    len(self._hd[t])))))\n",
    "\n",
    "                    features = np.vectorize(lambda i: hd[i], otypes=[int])(hashes)\n",
    "\n",
    "                    # for each the new labels graph hash P and produce the feature vectors\n",
    "                    u = self.random_state_.randn(nnv)\n",
    "                    if self.take_cauchy_:\n",
    "                        # cauchy\n",
    "                        u = np.divide(u, self.random_state_.randn(nnv))\n",
    "\n",
    "                    u = np.hstack((self._u[t], u))\n",
    "\n",
    "                    # calculate hashes for the remaining\n",
    "                    hashes = self.calculate_LSH(P[vertices_p, :], u, self._b[t])\n",
    "                    hd = dict(chain(iteritems(hd), iter((j, i) for i, j in enumerate(hashes, len(hd)))))\n",
    "\n",
    "                    features_p = np.vectorize(lambda i: hd[i], otypes=[int])(hashes)\n",
    "\n",
    "                    # Accumulate the results\n",
    "                    for k in range(n):\n",
    "                        A = Counter(features[np.logical_and(\n",
    "                            indexes[k] <= vertices, vertices <= indexes[k+1])])\n",
    "                        B = Counter(features_p[np.logical_and(\n",
    "                            indexes[k] <= vertices_p, vertices_p <= indexes[k+1])])\n",
    "                        phi[k][t] = A + B\n",
    "\n",
    "                    # calculate the Propagation matrix if needed\n",
    "                    if t < self.t_max-1:\n",
    "                        for k in range(n):\n",
    "                            start, end = indexes[k:k+2]\n",
    "                            P[start:end, :] = np.dot(transition_matrix[k], P[start:end, :])\n",
    "\n",
    "                        Q = np.all(P[:, dim_orig:] > 0, axis=1)\n",
    "                        vertices = np.where(~Q)[0]\n",
    "                        vertices_p = np.where(Q)[0]\n",
    "\n",
    "                return [phi[k] for k in range(n)]\n",
    "\n",
    "    def calculate_LSH(self, X, u, b):\n",
    "        \"\"\"Calculate Local Sensitive Hashing needed for propagation kernels.\n",
    "        See :cite:`neumann2015propagation`, p.12.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            A float array of shape (N, D) with N vertices and D features.\n",
    "        u : np.array, shape=(D, 1)\n",
    "            A projection vector.\n",
    "        b : float\n",
    "            An offset (times w).\n",
    "        Returns\n",
    "        -------\n",
    "        lsh : np.array.\n",
    "            The local sensitive hash coresponding to each vertex.\n",
    "        \"\"\"\n",
    "        if self.take_sqrt_:\n",
    "            X = np.sqrt(X)\n",
    "\n",
    "        # hash\n",
    "        return np.floor((np.dot(X, u)+b)/self.w)\n",
    "\n",
    "class PropagationAttr(Propagation):\n",
    "    r\"\"\"The Propagation kernel for fully attributed graphs.\n",
    "    See :cite:`neumann2015propagation`: Algorithms 1, 3, p. 216, 221.\n",
    "    Parameters\n",
    "    ----------\n",
    "    t_max : int, default=5\n",
    "        Maximum number of iterations.\n",
    "    w : int, default=0.01\n",
    "        Bin width.\n",
    "    M : str, default=\"TV\"\n",
    "        The preserved distance metric (on local sensitive hashing):\n",
    "            - \"L1\": l1-norm\n",
    "            - \"L2\": l2-norm\n",
    "    metric : function (np.array, np.array -> number),\n",
    "        default=:math:`f(x,y)=\\sum_{i} x_{i}*y_{i}`\n",
    "        A metric between two 1-dimensional numpy arrays of numbers\n",
    "        that outputs a number.\n",
    "    Attributes\n",
    "    ----------\n",
    "    M : str\n",
    "        The preserved distance metric (on local sensitive hashing).\n",
    "    tmax : int\n",
    "        Holds the maximum number of iterations.\n",
    "    w : int\n",
    "        Holds the bin width.\n",
    "    metric : function (np.array, np.array -> number)\n",
    "        A metric between two 1-dimensional numpy arrays of numbers\n",
    "        that outputs a number.\n",
    "    \"\"\"\n",
    "\n",
    "    _graph_format = \"adjacency\"\n",
    "    attr_ = True\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_jobs=None,\n",
    "                 verbose=False,\n",
    "                 normalize=False,\n",
    "                 random_state=None,\n",
    "                 metric=_dot,\n",
    "                 M=\"L1\",\n",
    "                 t_max=5,\n",
    "                 w=4):\n",
    "        \"\"\"Initialise a propagation kernel.\"\"\"\n",
    "        super(PropagationAttr, self).__init__(n_jobs=n_jobs,\n",
    "                                              verbose=verbose,\n",
    "                                              normalize=normalize,\n",
    "                                              random_state=random_state,\n",
    "                                              metric=metric,\n",
    "                                              M=M,\n",
    "                                              t_max=t_max,\n",
    "                                              w=w)\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize all transformer arguments, needing initialization.\"\"\"\n",
    "        super(PropagationAttr, self).initialize()\n",
    "\n",
    "    def parse_input(self, X):\n",
    "        \"\"\"Parse and create features for the attributed propation kernel.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : iterable\n",
    "            For the input to pass the test, we must have:\n",
    "            Each element must be an iterable with at most three features and at\n",
    "            least one. The first that is obligatory is a valid graph structure\n",
    "            (adjacency matrix or edge_dictionary) while the second is\n",
    "            node_labels and the third edge_labels (that correspond to the given\n",
    "            graph format). A valid input also consists of graph type objects.\n",
    "        Returns\n",
    "        -------\n",
    "        local_values : dict\n",
    "            A dictionary of pairs between each input graph and a bins where the\n",
    "            sampled graphlets have fallen.\n",
    "        \"\"\"\n",
    "        if not isinstance(X, collections.Iterable):\n",
    "            raise ValueError('input must be an iterable\\n')\n",
    "        else:\n",
    "            # The number of parsed graphs\n",
    "            n = 0\n",
    "            transition_matrix = dict()\n",
    "            indexes = [0]\n",
    "            Attr = list()\n",
    "            for (idx, x) in enumerate(iter(X)):\n",
    "                is_iter = isinstance(x, collections.Iterable)\n",
    "                if is_iter:\n",
    "                    x = list(x)\n",
    "                if is_iter and len(x) in [0, 2, 3, 4]:\n",
    "                    if len(x) == 0:\n",
    "                        warnings.warn('Ignoring empty element on ' +\n",
    "                                      'index: '+str(idx))\n",
    "                        continue\n",
    "                    if len(x) == 2 and type(x[0]) is Graph:\n",
    "                        g, T = x\n",
    "                    else:\n",
    "                        g = Graph(x[0], x[1], {}, self._graph_format)\n",
    "                        if len(x) == 4:\n",
    "                            T = x[3]\n",
    "                        else:\n",
    "                            T = None\n",
    "                elif type(x) is Graph:\n",
    "                    g, T = x, None\n",
    "                else:\n",
    "                    raise ValueError('Each element of X must be either a ' +\n",
    "                                     'Graph or an iterable with at least 2 ' +\n",
    "                                     'and at most 4 elements\\n')\n",
    "\n",
    "                if T is not None:\n",
    "                    if T.shape[0] != T.shape[1]:\n",
    "                        raise TypeError('Transition matrix on index' +\n",
    "                                        ' ' + str(idx) + 'must be ' +\n",
    "                                        'a square matrix.')\n",
    "                    if T.shape[0] != g.nv():\n",
    "                        raise TypeError('Propagation matrix must ' +\n",
    "                                        'have the same dimension ' +\n",
    "                                        'as the number of vertices.')\n",
    "                else:\n",
    "                    T = g.get_adjacency_matrix()\n",
    "\n",
    "                nv = g.nv()\n",
    "                transition_matrix[n] = normalizer(T, axis=1, norm='l1')\n",
    "                attr = g.get_labels(purpose=\"adjacency\")\n",
    "                try:\n",
    "                    attributes = np.array([attr[j] for j in range(nv)])\n",
    "                except TypeError:\n",
    "                    raise TypeError('All attributes of a single graph should have the same dimension.')\n",
    "\n",
    "                Attr.append(attributes)\n",
    "                indexes.append(indexes[-1] + nv)\n",
    "                n += 1\n",
    "            try:\n",
    "                P = np.vstack(Attr)\n",
    "            except ValueError:\n",
    "                raise ValueError('Attribute dimensions should be the same, for all graphs')\n",
    "\n",
    "            if self._method_calling == 1:\n",
    "                self._dim = P.shape[1]\n",
    "            else:\n",
    "                if self._dim != P.shape[1]:\n",
    "                    raise ValueError('transform attribute vectors should'\n",
    "                                     'have the same dimension as in fit')\n",
    "\n",
    "            if n == 0:\n",
    "                raise ValueError('Parsed input is empty')\n",
    "\n",
    "            # feature vectors\n",
    "            if self._method_calling == 1:\n",
    "                # simple normal\n",
    "                self._u, self._b, self._hd = list(), list(), list()\n",
    "                for t in range(self.t_max):\n",
    "                    u = self.random_state_.randn(self._dim)\n",
    "                    if self.take_cauchy_:\n",
    "                        # cauchy\n",
    "                        u = np.divide(u, self.random_state_.randn(self._dim))\n",
    "\n",
    "                    self._u.append(u)\n",
    "                    # random offset\n",
    "                    self._b.append(self.w*self.random_state_.randn(self._dim))\n",
    "\n",
    "                phi = {k: dict() for k in range(n)}\n",
    "                for t in range(self.t_max):\n",
    "                    # for hash all graphs inside P and produce the feature vectors\n",
    "                    hashes = self.calculate_LSH(P, self._u[t], self._b[t]).tolist()\n",
    "\n",
    "                    hd = {j: i for i, j in enumerate({tuple(l) for l in hashes})}\n",
    "                    self._hd.append(hd)\n",
    "\n",
    "                    features = np.array([hd[tuple(l)] for l in hashes])\n",
    "\n",
    "                    # Accumulate the results.\n",
    "                    for k in range(n):\n",
    "                        phi[k][t] = Counter(features[indexes[k]:indexes[k+1]].flat)\n",
    "\n",
    "                    # calculate the Propagation matrix if needed\n",
    "                    if t < self.t_max-1:\n",
    "                        for k in range(n):\n",
    "                            start, end = indexes[k:k+2]\n",
    "                            P[start:end, :] = np.dot(transition_matrix[k], P[start:end, :])\n",
    "\n",
    "                return [phi[k] for k in range(n)]\n",
    "\n",
    "            if self._method_calling == 3:\n",
    "                phi = {k: dict() for k in range(n)}\n",
    "                for t in range(self.t_max):\n",
    "                    # for hash all graphs inside P and produce the feature vectors\n",
    "                    hashes = self.calculate_LSH(P, self._u[t], self._b[t]).tolist()\n",
    "\n",
    "                    hd = dict(chain(\n",
    "                            iteritems(self._hd[t]),\n",
    "                            iter((j, i) for i, j in enumerate(\n",
    "                                    filterfalse(lambda x: x in self._hd[t],\n",
    "                                                {tuple(l) for l in hashes}),\n",
    "                                    len(self._hd[t])))))\n",
    "\n",
    "                    features = np.array([hd[tuple(l)] for l in hashes])\n",
    "\n",
    "                    # Accumulate the results.\n",
    "                    for k in range(n):\n",
    "                        phi[k][t] = Counter(features[indexes[k]:indexes[k+1]])\n",
    "\n",
    "                    # calculate the Propagation matrix if needed\n",
    "                    if t < self.t_max-1:\n",
    "                        for k in range(n):\n",
    "                            start, end = indexes[k:k+2]\n",
    "                            P[start:end, :] = np.dot(transition_matrix[k], P[start:end, :])\n",
    "\n",
    "                return [phi[k] for k in range(n)]\n",
    "\n",
    "    def calculate_LSH(self, X, u, b):\n",
    "        \"\"\"Calculate Local Sensitive Hashing needed for propagation kernels.\n",
    "        See :cite:`neumann2015propagation`, p.12.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            A float array of shape (N, D) with N vertices and D features.\n",
    "        u : np.array, shape=(D, 1)\n",
    "            A projection vector.\n",
    "        b : float\n",
    "            An offset (times w).\n",
    "        Returns\n",
    "        -------\n",
    "        lsh : np.array.\n",
    "            The local sensitive hash coresponding to each vertex.\n",
    "        \"\"\"\n",
    "        return np.floor((X*u+b)/self.w)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_gs_for_sp(Gs, round_edge, round_node):\n",
    "    \n",
    "    Gs_new  = []\n",
    "\n",
    "    max_weight = 0\n",
    "    for i in range(len(Gs)):\n",
    "        try:\n",
    "            max_weight_i = np.max([np.abs(w[2]) for w in Gs[i].edges(data = 'weight')])\n",
    "        except:\n",
    "            max_weight_i = 0\n",
    "        if max_weight_i > max_weight:\n",
    "            max_weight = np.abs(max_weight_i)\n",
    "\n",
    "    if max_weight == 0:\n",
    "        max_weight = 1\n",
    "\n",
    "    max_return = 0\n",
    "\n",
    "    for i in range(len(Gs)):\n",
    "        max_return_i = np.max([w[1] for w in Gs[i].nodes(data = 'attr')])\n",
    "        if max_weight_i > max_return:\n",
    "            max_return = np.abs(max_return_i)\n",
    "            \n",
    "    for i in range(len(Gs)):\n",
    "\n",
    "        Gs_new.append(nx.from_numpy_array(np.abs(np.round(nx.adjacency_matrix(Gs[i]).todense()/max_weight, round_edge))))\n",
    "        nx.set_node_attributes(Gs_new[i], {v:str(np.round(w[0]/max_return,round_node)) for v, w in nx.get_node_attributes(Gs[0], 'attr').items()}, 'label')\n",
    "\n",
    "    return Gs_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grakel as gk\n",
    "\n",
    "sys.path.insert(0, 'C:/Users/User/Code/MMD_Graph_Diversification/myKernels')\n",
    "from myKernels import RandomWalk as rw\n",
    "import MMDforGraphs as mg\n",
    "import WL\n",
    "import GNTK\n",
    "import GraphStatKernel\n",
    "import WWL\n",
    "import sp\n",
    "\n",
    "\n",
    "d = 1\n",
    "winow_len = 300\n",
    "graph_estimation = 'huge_glasso_ebic'\n",
    "edge_attr = 'weight'\n",
    "\n",
    "n = 20\n",
    "day_step = 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "study = 'Utilities'\n",
    "if study == 'all':\n",
    "    file = f'../data/Graphs/Striped_{study}_d_1_winlen_300_gest_huge_glasso_ebic_scale_{False}_trans_{\"nonparanormal\"}.pkl'\n",
    "else:\n",
    "    file = f'../data/Graphs/Striped_{study}_d_1_winlen_300_gest_huge_glasso_ebic_scale_{False}_trans_{\"nonparanormal\"}.pkl'\n",
    "with open(file, 'rb') as f:\n",
    "    data_dict = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "edge_attr = 'weight'\n",
    "\n",
    "transform = 'nonparanormal'\n",
    "scale =  False\n",
    "ptype =  'return'\n",
    "graph_name = 'graph_dict'\n",
    "graph_label = 'prop'\n",
    "do_tensor = False\n",
    "kernel_params = {'d':1, 'L':6, 'with_labels':False, 'c':1e-6, 't_max':6, 'w':0.0001, 'L':6, 'd':6, 'round_node':1, 'round_edge':2}\n",
    "B = 5000\n",
    "point = 500\n",
    "my_range = range(point, point+2, day_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = n\n",
    "\n",
    "graph_dict = data_dict[graph_name]\n",
    "k = data_dict['sector']\n",
    "dates = data_dict['dates']\n",
    "print(k)\n",
    "\n",
    "esg_return_df = pd.DataFrame()\n",
    "total_length = len(graph_dict[0])\n",
    "pbar = tqdm.tqdm( total=len(list(my_range))*1, desc= f'{study} {graph_name} {graph_label} {ptype}')\n",
    "for group_1 in [0]:#range(nr_splits):\n",
    "    for group_2 in [2]:#range(group_1+1, nr_splits):\n",
    "\n",
    "        for i in my_range:\n",
    "            # print(i-cnt)n\n",
    "            print(dates[i])\n",
    "\n",
    "            if graph_name == 'cov_dict':\n",
    "                Gs = [ nx.from_numpy_array(1e4*graph_dict[group_1][s]) for s in range(i-n, i )] + [ nx.from_numpy_array(1e4*graph_dict[group_2][s]) for s in range(i-n, i )]\n",
    "            else:\n",
    "                Gs = [ graph_dict[group_1][s] for s in range(i-n, i )] + [ graph_dict[group_2][s] for s in range(i-n, i )]\n",
    "            \n",
    "\n",
    "            # rw nr eigenvalues\n",
    "            r = np.min((6, Gs[0].number_of_nodes()-1))\n",
    "\n",
    "            # get attributes\n",
    "            if ptype is None:\n",
    "                p = None\n",
    "                q = None\n",
    "            elif ptype == 'return':\n",
    "                if graph_label == 'rw':\n",
    "                    p = np.vstack(([ data_dict['return_dict'][group_1][s]*1000 for s in range(i-n, i )],[ data_dict['return_dict'][group_2][s]*1000  for s in range(i-n, i )]))\n",
    "                else:\n",
    "                    p = np.vstack(([ data_dict['return_dict'][group_1][s] for s in range(i-n, i )],[ data_dict['return_dict'][group_2][s]  for s in range(i-n, i )]))\n",
    "                q = p.copy()\n",
    "            else:\n",
    "                p = np.vstack(([ np.ones(Gs[0].number_of_nodes())/float(Gs[0].number_of_nodes()) for s in range(i-n, i )],[ np.ones(Gs[0].number_of_nodes())/float(Gs[0].number_of_nodes()) for s in range(i-n, i )]))\n",
    "                q = p.copy()\n",
    "\n",
    "            for k in range(len(Gs)):\n",
    "                nx.set_node_attributes(Gs[k], {j:[p[k,j]] for j in range(Gs[k].number_of_nodes())}, 'attr')\n",
    "                nx.set_node_attributes(Gs[k], {j:str(k) for j,k in Gs[k].degree}, \"label\")\n",
    "\n",
    "            if do_tensor:\n",
    "                Gs_plus = []\n",
    "                Gs_negative = []\n",
    "                for k in range(len(Gs)):\n",
    "                    A = nx.adjacency_matrix(Gs[k]).todense()\n",
    "                    A_plus = A.copy()\n",
    "                    A_plus[A_plus<0] =0\n",
    "                    Gs_plus.append(nx.from_numpy_array(A_plus))\n",
    "                    nx.set_node_attributes(Gs_plus[k], nx.get_node_attributes(Gs[k], 'attr'), \"attr\")\n",
    "                    nx.set_node_attributes(Gs_plus[k], {j:str(k) for j,k in Gs[k].degree}, \"label\")\n",
    "\n",
    "\n",
    "                    A_negative = A.copy()\n",
    "                    A_negative[A_negative>0] =0\n",
    "                    A_negative = np.abs(A_negative)\n",
    "                    Gs_negative.append(nx.from_numpy_array(A_negative))\n",
    "                    nx.set_node_attributes(Gs_negative[k], nx.get_node_attributes(Gs[k], 'attr'), \"attr\")\n",
    "                    nx.set_node_attributes(Gs_negative[k], {j:str(k) for j,k in Gs[k].degree}, \"label\")\n",
    "            \n",
    "            Gs_abs = []\n",
    "            for k in range(len(Gs)):\n",
    "                A = nx.adjacency_matrix(Gs[k]).todense()\n",
    "                Gs_abs.append(nx.from_numpy_array(np.abs(A.copy())))\n",
    "                nx.set_node_attributes(Gs_abs[k], nx.get_node_attributes(Gs[k], 'attr'), \"attr\")\n",
    "                nx.set_node_attributes(Gs_abs[k], {j:str(k) for j,k in Gs[k].degree}, \"label\")\n",
    "\n",
    "\n",
    "            if graph_label  == 'rw':\n",
    "                calc_ok = False\n",
    "                c_new = kernel_params['c']\n",
    "                while not calc_ok:\n",
    "                    calc_ok = True\n",
    "                    try:\n",
    "                        rw_kernel = rw.RandomWalk(Gs, c = c_new, normalize=0, p=p, q = q)\n",
    "                        K = rw_kernel.fit_ARKU_plus(r = r, normalize_adj=False, verbose=False, edge_attr = edge_attr)\n",
    "\n",
    "                        v,_ = np.linalg.eigh(K)\n",
    "                        # v[np.abs(v) < 10e-5] = 0\n",
    "                        if np.any(v < -10e-12):\n",
    "                            raise ValueError(\"Not psd\")\n",
    "                    except:\n",
    "                        calc_ok = False\n",
    "                        c_new = c_new*0.8\n",
    "                        print(f'{study} {graph_name} {graph_label} {ptype} {dates[i]} new c is {c_new}')\n",
    "            \n",
    "            if graph_label  == 'rw_scaled':\n",
    "            \n",
    "                for idx_g in range(len(Gs)):\n",
    "                    scale_edge = np.std([w[2] for w in Gs[idx_g].edges(data = 'weight')])\n",
    "                    nx.set_edge_attributes(Gs[idx_g],{(w[0], w[1]): w[2]/scale_edge for w in Gs[idx_g].edges(data = 'weight')}, 'weight')\n",
    "\n",
    "                    scale_node = np.std([w[1][0] for w in Gs[idx_g].nodes(data = 'attr')])\n",
    "                    nx.set_node_attributes(Gs[idx_g],{w[0]: w[1]/scale_node for w in Gs[idx_g].nodes(data = 'attr')}, 'attr')\n",
    "\n",
    "                calc_ok = False\n",
    "                c_new = kernel_params['c']\n",
    "                while not calc_ok:\n",
    "                    calc_ok = True\n",
    "                    try:\n",
    "                        rw_kernel = rw.RandomWalk(Gs, c = c_new, normalize=0, p=p, q = q)\n",
    "                        K = rw_kernel.fit_ARKU_plus(r = r, normalize_adj=False, verbose=False, edge_attr = edge_attr)\n",
    "\n",
    "                        v,_ = np.linalg.eigh(K)\n",
    "                        # v[np.abs(v) < 10e-5] = 0\n",
    "                        if np.any(v < -10e-12):\n",
    "                            raise ValueError(\"Not psd\")\n",
    "                    except:\n",
    "                        calc_ok = False\n",
    "                        c_new = c_new*0.8\n",
    "                        print(f'{study} {graph_name} {graph_label} {ptype} {dates[i]} new c is {c_new}')\n",
    "            elif graph_label == 'wl':\n",
    "                if do_tensor:\n",
    "                    kernel = [{\"name\": \"weisfeiler_lehman\", \"n_iter\":kernel_params['h']}, {\"name\": \"vertex_histogram\"}]\n",
    "                    init_kernel = gk.GraphKernel(kernel= kernel, normalize=0)\n",
    "                    graph_list = gk.graph_from_networkx(Gs_plus, node_labels_tag='label')  # Convert to graphs to Grakel format\n",
    "                    K_plus = init_kernel.fit_transform(graph_list)\n",
    "\n",
    "                    kernel = [{\"name\": \"weisfeiler_lehman\", \"n_iter\":kernel_params['h']}, {\"name\": \"vertex_histogram\"}]\n",
    "                    init_kernel = gk.GraphKernel(kernel= kernel, normalize=0)\n",
    "                    graph_list = gk.graph_from_networkx(Gs_negative, node_labels_tag='label')  # Convert to graphs to Grakel format\n",
    "                    K_negative = init_kernel.fit_transform(graph_list)\n",
    "\n",
    "                    K = np.multiply(K_plus, K_negative)\n",
    "\n",
    "                else:\n",
    "                    kernel = [{\"name\": \"weisfeiler_lehman\", \"n_iter\":kernel_params['h']}, {\"name\": \"vertex_histogram\"}]\n",
    "                    init_kernel = gk.GraphKernel(kernel= kernel, normalize=0)\n",
    "                    graph_list = gk.graph_from_networkx(Gs, node_labels_tag='label')  # Convert to graphs to Grakel format\n",
    "                    K = init_kernel.fit_transform(graph_list)\n",
    "            \n",
    "            elif graph_label == 'wl_bin':\n",
    "                if do_tensor:\n",
    "                    \n",
    "                    Gs_plus = prepare_gs_for_sp(Gs_plus, round_node= kernel_params['round_node'], round_edge= 2)\n",
    "                    Gs_negative = prepare_gs_for_sp(Gs_negative, round_node= kernel_params['round_node'], round_edge=2)\n",
    "\n",
    "                    kernel = [{\"name\": \"weisfeiler_lehman\", \"n_iter\":kernel_params['h']}, {\"name\": \"vertex_histogram\"}]\n",
    "                    init_kernel = gk.GraphKernel(kernel= kernel, normalize=0)\n",
    "                    graph_list = gk.graph_from_networkx(Gs_plus, node_labels_tag='label')  # Convert to graphs to Grakel format\n",
    "                    K_plus = init_kernel.fit_transform(graph_list)\n",
    "\n",
    "                    kernel = [{\"name\": \"weisfeiler_lehman\", \"n_iter\":kernel_params['h']}, {\"name\": \"vertex_histogram\"}]\n",
    "                    init_kernel = gk.GraphKernel(kernel= kernel, normalize=0)\n",
    "                    graph_list = gk.graph_from_networkx(Gs_negative, node_labels_tag='label')  # Convert to graphs to Grakel format\n",
    "                    K_negative = init_kernel.fit_transform(graph_list)\n",
    "\n",
    "                    K = np.multiply(K_plus, K_negative)\n",
    "\n",
    "                else:\n",
    "                    Gs = prepare_gs_for_sp(Gs, round_node= kernel_params['round_node'], round_edge= 2)\n",
    "                    kernel = [{\"name\": \"weisfeiler_lehman\", \"n_iter\":kernel_params['h']}, {\"name\": \"vertex_histogram\"}]\n",
    "                    init_kernel = gk.GraphKernel(kernel= kernel, normalize=0)\n",
    "                    graph_list = gk.graph_from_networkx(Gs, node_labels_tag='label')  # Convert to graphs to Grakel format\n",
    "                    K = init_kernel.fit_transform(graph_list)\n",
    "\n",
    "            elif graph_label == 'wloa':\n",
    "                if do_tensor:\n",
    "                    kernel = [{\"name\": \"WL-OA\", \"n_iter\": kernel_params['h']}]\n",
    "                    init_kernel = gk.GraphKernel(kernel= kernel, normalize=0)\n",
    "                    graph_list = gk.graph_from_networkx(Gs_plus, node_labels_tag='label')  # Convert to graphs to Grakel format\n",
    "                    K_plus = init_kernel.fit_transform(graph_list)\n",
    "\n",
    "                    kernel = [{\"name\": \"WL-OA\", \"n_iter\": kernel_params['h']}]\n",
    "                    init_kernel = gk.GraphKernel(kernel= kernel, normalize=0)\n",
    "                    graph_list = gk.graph_from_networkx(Gs_negative, node_labels_tag='label')  # Convert to graphs to Grakel format\n",
    "                    K_negative = init_kernel.fit_transform(graph_list)\n",
    "\n",
    "                    K = np.multiply(K_plus, K_negative)\n",
    "                else:\n",
    "                    kernel = [{\"name\": \"WL-OA\", \"n_iter\": kernel_params['h']}]\n",
    "                    init_kernel = gk.GraphKernel(kernel= kernel, normalize=0)\n",
    "                    graph_list = gk.graph_from_networkx(Gs, node_labels_tag='label')  # Convert to graphs to Grakel format\n",
    "                    K = init_kernel.fit_transform(graph_list)\n",
    "\n",
    "            elif graph_label == 'wloa_bin':\n",
    "                if do_tensor:\n",
    "\n",
    "                    Gs_plus = prepare_gs_for_sp(Gs_plus, round_node= kernel_params['round_node'], round_edge= 2)\n",
    "                    Gs_negative = prepare_gs_for_sp(Gs_negative, round_node= kernel_params['round_node'], round_edge=2)\n",
    "\n",
    "                    kernel = [{\"name\": \"WL-OA\", \"n_iter\": kernel_params['h']}]\n",
    "                    init_kernel = gk.GraphKernel(kernel= kernel, normalize=0)\n",
    "                    graph_list = gk.graph_from_networkx(Gs_plus, node_labels_tag='label')  # Convert to graphs to Grakel format\n",
    "                    K_plus = init_kernel.fit_transform(graph_list)\n",
    "\n",
    "                    kernel = [{\"name\": \"WL-OA\", \"n_iter\": kernel_params['h']}]\n",
    "                    init_kernel = gk.GraphKernel(kernel= kernel, normalize=0)\n",
    "                    graph_list = gk.graph_from_networkx(Gs_negative, node_labels_tag='label')  # Convert to graphs to Grakel format\n",
    "                    K_negative = init_kernel.fit_transform(graph_list)\n",
    "\n",
    "                    K = np.multiply(K_plus, K_negative)\n",
    "                else:\n",
    "                    Gs = prepare_gs_for_sp(Gs, round_node= kernel_params['round_node'], round_edge= 2)\n",
    "                    kernel = [{\"name\": \"WL-OA\", \"n_iter\": kernel_params['h']}]\n",
    "                    init_kernel = gk.GraphKernel(kernel= kernel, normalize=0)\n",
    "                    graph_list = gk.graph_from_networkx(Gs, node_labels_tag='label')  # Convert to graphs to Grakel format\n",
    "                    K = init_kernel.fit_transform(graph_list)\n",
    "            elif graph_label == \"wwl\":\n",
    "\n",
    "                if do_tensor:\n",
    "                    kernel = WWL.WWL(param = {'discount':kernel_params['w'],'h':kernel_params['h'], 'sinkhorn':False })\n",
    "                    K_plus = kernel.fit_transform(Gs_plus)\n",
    "\n",
    "                    kernel = WWL.WWL(param = {'discount':kernel_params['w'],'h':kernel_params['h'], 'sinkhorn':False })\n",
    "                    K_negative = kernel.fit_transform(Gs_negative)\n",
    "\n",
    "                    K = np.multiply(K_plus, K_negative)\n",
    "                else:\n",
    "                    kernel = WWL.WWL(param = {'discount':kernel_params['w'],'h':kernel_params['h'], 'sinkhorn':False })\n",
    "                    K = kernel.fit_transform(Gs_abs)\n",
    "\n",
    "            elif graph_label == \"wwl_bin\":\n",
    "\n",
    "                if do_tensor:\n",
    "\n",
    "                    Gs_plus = prepare_gs_for_sp(Gs_plus, round_node= kernel_params['round_node'], round_edge= 2)\n",
    "                    Gs_negative = prepare_gs_for_sp(Gs_negative, round_node= kernel_params['round_node'], round_edge=2)\n",
    "\n",
    "                    kernel = WWL.WWL(param = {'discount':kernel_params['w'],'h':kernel_params['h'], 'sinkhorn':False })\n",
    "                    K_plus = kernel.fit_transform(Gs_plus)\n",
    "\n",
    "                    kernel = WWL.WWL(param = {'discount':kernel_params['w'],'h':kernel_params['h'], 'sinkhorn':False })\n",
    "                    K_negative = kernel.fit_transform(Gs_negative)\n",
    "\n",
    "                    K = np.multiply(K_plus, K_negative)\n",
    "                else:\n",
    "                    Gs = prepare_gs_for_sp(Gs, round_node= kernel_params['round_node'], round_edge= 2)\n",
    "                    kernel = WWL.WWL(param = {'discount':kernel_params['w'],'h':kernel_params['h'], 'sinkhorn':False })\n",
    "                    K = kernel.fit_transform(Gs)\n",
    "\n",
    "            elif graph_label == 'pyramid':\n",
    "                if do_tensor:\n",
    "                    pm = gk.PyramidMatch(with_labels = kernel_params['with_labels'], L = kernel_params['L'], d = kernel_params['d'])\n",
    "                    gk_gs = gk.graph_from_networkx(Gs_plus, edge_weight_tag='weight',  node_labels_tag = 'label')\n",
    "                    K_plus = pm.fit_transform(gk_gs)\n",
    "\n",
    "                    pm = gk.PyramidMatch(with_labels = kernel_params['with_labels'], L = kernel_params['L'], d = kernel_params['d'])\n",
    "                    gk_gs = gk.graph_from_networkx(Gs_negative, edge_weight_tag='weight',  node_labels_tag = 'label')\n",
    "                    K_negative = pm.fit_transform(gk_gs)\n",
    "\n",
    "                    K = np.multiply(K_plus, K_negative)\n",
    "                else:\n",
    "                    pm = gk.PyramidMatch(with_labels = kernel_params['with_labels'], L = kernel_params['L'], d = kernel_params['d'])\n",
    "                    gk_gs = gk.graph_from_networkx(Gs, edge_weight_tag='weight',  node_labels_tag = 'label')\n",
    "                    K = pm.fit_transform(gk_gs)\n",
    "\n",
    "            elif graph_label == 'prop':\n",
    "                if do_tensor:\n",
    "                    prop = PropagationAttr(w = kernel_params['w'],t_max = kernel_params['t_max'])\n",
    "                    gk_gs = gk.graph_from_networkx(Gs_plus, edge_weight_tag='weight',  node_labels_tag = 'attr')\n",
    "                    K_plus = prop.fit_transform(gk_gs)\n",
    "\n",
    "                    prop = PropagationAttr(w = kernel_params['w'],t_max = kernel_params['t_max'])\n",
    "                    gk_gs = gk.graph_from_networkx(Gs_negative, edge_weight_tag='weight',  node_labels_tag = 'attr')\n",
    "                    K_negative = prop.fit_transform(gk_gs)\n",
    "\n",
    "                    K = np.multiply(K_plus, K_negative)\n",
    "                else:\n",
    "                    prop = PropagationAttr(w = kernel_params['w'],t_max = kernel_params['t_max'])\n",
    "                    gk_gs = gk.graph_from_networkx(Gs_abs, edge_weight_tag='weight',  node_labels_tag = 'attr')\n",
    "                    K = prop.fit_transform(gk_gs)\n",
    "\n",
    "            elif graph_label == 'sp_attr':\n",
    "                if do_tensor:\n",
    "                    Gs_sp_plus = prepare_gs_for_sp(Gs_plus, round_node= kernel_params['round_node'], round_edge= kernel_params['round_edge'])\n",
    "                    Gs_sp_negative = prepare_gs_for_sp(Gs_negative, round_node= kernel_params['round_node'], round_edge= kernel_params['round_edge'])\n",
    "\n",
    "                    init_kernel = gk.ShortestPath(normalize=0, with_labels=True)\n",
    "                    graph_list = gk.graph_from_networkx(Gs_sp_plus, edge_weight_tag='weight', node_labels_tag= 'label')  # Convert to graphs to Grakel format\n",
    "                    K_plus = init_kernel.fit_transform(graph_list)\n",
    "\n",
    "                    init_kernel = gk.ShortestPath(normalize=0, with_labels=True)\n",
    "                    graph_list = gk.graph_from_networkx(Gs_sp_negative, edge_weight_tag='weight', node_labels_tag= 'label')  # Convert to graphs to Grakel format\n",
    "                    K_negative = init_kernel.fit_transform(graph_list)\n",
    "\n",
    "                    K = np.multiply(K_plus, K_negative)\n",
    "                else:\n",
    "                    Gs_abs_sp = prepare_gs_for_sp(Gs_abs, round_node= kernel_params['round_node'], round_edge= kernel_params['round_edge'])\n",
    "                    init_kernel = gk.ShortestPath(normalize=0, with_labels=True)\n",
    "                    graph_list = gk.graph_from_networkx(Gs_abs_sp, edge_weight_tag='weight', node_labels_tag= 'label')  # Convert to graphs to Grakel format\n",
    "                    K = init_kernel.fit_transform(graph_list)\n",
    "\n",
    "            elif graph_label == 'sp':\n",
    "                if do_tensor:\n",
    "                    Gs_sp_plus = prepare_gs_for_sp(Gs_plus, round_node= kernel_params['round_node'], round_edge= kernel_params['round_edge'])\n",
    "                    Gs_sp_negative = prepare_gs_for_sp(Gs_negative, round_node= kernel_params['round_node'], round_edge= kernel_params['round_edge'])\n",
    "\n",
    "                    init_kernel = gk.ShortestPath(normalize=0, with_labels=False)\n",
    "                    graph_list = gk.graph_from_networkx(Gs_sp_plus, edge_weight_tag='weight')  # Convert to graphs to Grakel format\n",
    "                    K_plus = init_kernel.fit_transform(graph_list)\n",
    "\n",
    "                    init_kernel = gk.ShortestPath(normalize=0, with_labels=False)\n",
    "                    graph_list = gk.graph_from_networkx(Gs_sp_negative, edge_weight_tag='weight')  # Convert to graphs to Grakel format\n",
    "                    K_negative = init_kernel.fit_transform(graph_list)\n",
    "\n",
    "                    K = np.multiply(K_plus, K_negative)\n",
    "                else:\n",
    "                    Gs_abs_sp = prepare_gs_for_sp(Gs_abs, round_node= kernel_params['round_node'], round_edge= kernel_params['round_edge'])\n",
    "                    init_kernel = gk.ShortestPath(normalize=0, with_labels=False)\n",
    "                    graph_list = gk.graph_from_networkx(Gs_abs_sp, edge_weight_tag='weight')  # Convert to graphs to Grakel format\n",
    "                    K = init_kernel.fit_transform(graph_list)\n",
    "\n",
    "\n",
    "            else:\n",
    "                ValueError(f\"Check if graph_label is written correctly\")\n",
    "\n",
    "            MMD_functions = [mg.MMD_b, mg.MMD_u, mg.MMD_l]#, mg.MONK_EST]\n",
    "            kernel_hypothesis = mg.BoostrapMethods(MMD_functions)\n",
    "            function_arguments = [dict(n1 = n, n2 = m ), \n",
    "                                dict(n1 = n, n2 = m ),\n",
    "                                dict(n1 = n, n2 = m )]#, \n",
    "                                #dict(Q = 5, n1 = n, n2 = m  )]\n",
    "            kernel_hypothesis.Bootstrap(K, function_arguments, B = B)\n",
    "\n",
    "            info_dict = dict()\n",
    "            info_dict['sector'] = k\n",
    "            info_dict['group_i'] = group_1\n",
    "            info_dict['group_j'] = group_2\n",
    "            info_dict['MMD_u'] = kernel_hypothesis.p_values['MMD_u']\n",
    "            info_dict['MMD_b'] = kernel_hypothesis.p_values['MMD_b']\n",
    "            info_dict['MMD_l'] = kernel_hypothesis.p_values['MMD_l']\n",
    "            #info_dict['MONK_EST'] = kernel_hypothesis.p_values['MONK_EST']\n",
    "            info_dict['kernel'] = \"rw\"\n",
    "            info_dict['r'] = r\n",
    "            info_dict['dates'] = dates[i]\n",
    "            info_dict['dates_mid'] = dates[int((i+(i-n))/2)]\n",
    "\n",
    "            for k, v in kernel_params.items():\n",
    "                info_dict[k] = v\n",
    "\n",
    "            esg_return_df = pd.concat((esg_return_df, pd.DataFrame(info_dict, index = [0])), ignore_index=True)\n",
    "\n",
    "            pbar.update()\n",
    "pbar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(K_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(K_negative)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "699b387727d3e14b4e8adcaa5733ffdb4eff0f6be32e5f0416e8760a070195e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
